{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "611dcb3b",
      "metadata": {
        "id": "611dcb3b"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c3YY76TpwIiP",
      "metadata": {
        "id": "c3YY76TpwIiP"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    n_emb: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Dense(4 * self.n_emb)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(self.n_emb)(x) # FOR RESIDUAL PATHWAY\n",
        "        return x\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "    head_size: int\n",
        "\n",
        "    def setup(self):\n",
        "        self.causal_mask = jnp.tril(jnp.ones((block_size, block_size), dtype=bool))\n",
        "\n",
        "        self.to_q = nn.Dense(self.head_size, use_bias=False) # (C, hs)\n",
        "        self.to_k = nn.Dense(self.head_size, use_bias=False) # (C, hs)\n",
        "        self.to_v = nn.Dense(self.head_size, use_bias=False) # (C, hs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q = self.to_q(x) # (B, T, C) @ (C, hs) -> (B, T, hs)\n",
        "        k = self.to_k(x) # (B, T, C) @ (C, hs) -> (B, T, hs)\n",
        "        v = self.to_v(x) # (B, T, C) @ (C, hs) -> (B, T, hs)\n",
        "\n",
        "        attn_scores = (q @ k.swapaxes(-2, -1)) * (self.head_size ** -0.5) # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "\n",
        "        mask = self.causal_mask[:T, :T]\n",
        "        attn_scores = jnp.where(~mask[None, :, :], -jnp.inf, attn_scores)\n",
        "        attn_weights = nn.softmax(attn_scores, axis=-1)\n",
        "        return attn_weights @ v # (B, T, T) @ (B, T, hs) ---> (B, T, hs)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    num_heads: int\n",
        "    head_size: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        out = jnp.concatenate([AttentionHead(self.head_size)(x) for _ in range(self.num_heads)], axis=-1) # (B, T, hs*num_heads)\n",
        "        out = nn.Dense(self.num_heads*self.head_size)(out) # FOR RESIDUAL PATHWAY\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    n_emb: int\n",
        "    num_heads: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        # NOTE the x + (stuff) is a RESIDUAL CONNECTION\n",
        "        x = x + MultiHeadAttention(self.num_heads, self.n_emb//self.num_heads)(nn.LayerNorm()(x)) # (B, T, C) num_heads of (n_emb//num_heads)-dimensional self-attention\n",
        "        x = x + FeedForward(self.n_emb)(nn.LayerNorm()(x)) # (B, T, C)\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    n_emb: int\n",
        "    vocab_size: int\n",
        "    block_size: int\n",
        "    num_heads: int\n",
        "    num_blocks: int\n",
        "\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, idx):\n",
        "        B, T = idx.shape\n",
        "        x = nn.Embed(self.vocab_size, self.n_emb)(idx) # (B, T, C)\n",
        "        # positional embedding layer\n",
        "        x += nn.Embed(self.block_size, self.n_emb)(jnp.arange(T)) # (B, T, C) + (T, C) (broadcast)\n",
        "        blocks = nn.Sequential([Block(self.n_emb, self.num_heads) for _ in range(self.num_blocks)])\n",
        "        x = blocks(x)\n",
        "        logits = nn.Dense(self.vocab_size)(nn.LayerNorm()(x)) # (B, T, vocab_size)\n",
        "        return logits\n",
        "\n",
        "    def generate(self, key, params, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size:] # crop idx so that it's never bigger than block_size (B, T)\n",
        "            logits = self.apply(params, idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            key, subkey = jax.random.split(key)\n",
        "            idx_next = jax.random.categorical(subkey, logits, shape=(logits.shape[0],1))\n",
        "            idx = jnp.concatenate([idx, idx_next], axis=1).astype(int)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dOAkxy9-wTqq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOAkxy9-wTqq",
        "outputId": "ede9f105-3879-4fab-874c-d521a7de9b44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-24 22:35:15--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-24 22:35:15 (16.8 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# we always start with a dataset to train on. let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "33fb1dcb",
      "metadata": {
        "id": "33fb1dcb"
      },
      "outputs": [],
      "source": [
        "with open('smiles.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = {c:i for i,c in enumerate(chars)}\n",
        "itos = {i:c for i,c in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "data = jnp.array(encode(text), dtype=jnp.int32)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "8c21d058",
      "metadata": {
        "id": "8c21d058"
      },
      "outputs": [],
      "source": [
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 128 # what is the maximum context length for predictions?\n",
        "n_emb = 384\n",
        "num_heads = 6\n",
        "num_blocks = 6\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "13933cdd",
      "metadata": {
        "id": "13933cdd"
      },
      "outputs": [],
      "source": [
        "def get_batch(key, data):\n",
        "    # how many valid start positions we have\n",
        "    max_idx = data.shape[0] - (block_size + 1)\n",
        "    # draw batch_size random starts in [0, max_idx)\n",
        "    starts = jax.random.randint(key, (batch_size,), 0, max_idx)\n",
        "\n",
        "    def slice_pair(i):\n",
        "        # pull [i : i+block_size]  and [i+1 : i+1+block_size]\n",
        "        x = jax.lax.dynamic_slice(data, (i,), (block_size,))\n",
        "        y = jax.lax.dynamic_slice(data, (i+1,), (block_size,))\n",
        "        return x, y\n",
        "\n",
        "    # vmap over our vector of starts\n",
        "    xs, ys = jax.vmap(slice_pair)(starts)\n",
        "    return xs, ys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "31aab1d3",
      "metadata": {
        "id": "31aab1d3"
      },
      "outputs": [],
      "source": [
        "model = GPT(n_emb=n_emb, vocab_size=vocab_size, block_size=block_size, num_heads=num_heads, num_blocks=num_blocks)\n",
        "\n",
        "def loss_fn(params, x, y):\n",
        "    logits = model.apply(params, x)\n",
        "    return optax.losses.softmax_cross_entropy_with_integer_labels(logits, y).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "3ccd1c83",
      "metadata": {
        "id": "3ccd1c83"
      },
      "outputs": [],
      "source": [
        "# Initialise Model ===============================\n",
        "key, init_key = jax.random.split(jax.random.PRNGKey(420696969))\n",
        "xb, yb = get_batch(key, train_data)\n",
        "params = model.init(init_key, jnp.ones_like(xb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "673912a3",
      "metadata": {
        "id": "673912a3"
      },
      "outputs": [],
      "source": [
        "#TODO Make this faster\n",
        "def estimate_loss(key, params, eval_iters=200):\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = []\n",
        "        for k in range(eval_iters):\n",
        "            key, subkey = jax.random.split(key)\n",
        "            if split == 'train':\n",
        "                X, Y = get_batch(subkey, train_data)\n",
        "            else:\n",
        "                X, Y = get_batch(subkey, val_data)\n",
        "            loss = loss_fn(params, X, Y)\n",
        "            losses.append(float(loss))\n",
        "        out[split] = sum(losses) / len(losses)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "hNyC4uxS8ZPo",
      "metadata": {
        "id": "hNyC4uxS8ZPo"
      },
      "outputs": [],
      "source": [
        "tx = optax.adam(learning_rate=learning_rate)\n",
        "opt_state = tx.init(params)\n",
        "\n",
        "@jax.jit\n",
        "def train_step(opt_state, params, X, Y):\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params, X, Y)\n",
        "    updates, opt_state = tx.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return loss, params, opt_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f9dfb2e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9dfb2e4",
        "outputId": "0e16c859-d816-4ac8-c429-095d0387e681"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [04:56<00:00, 16.88it/s]\n"
          ]
        }
      ],
      "source": [
        "for i in tqdm(range(5000)):\n",
        "    key, train_key = jax.random.split(key)\n",
        "    xb, yb = get_batch(train_key, train_data)\n",
        "    loss, params, opt_state = train_step(opt_state, params, xb, yb)\n",
        "    if i % 1000 == 0:\n",
        "      pass\n",
        "        #key, est_key = jax.random.split(key)\n",
        "        #losses = estimate_loss(est_key, params)\n",
        "        #print(f\"Training loss: {losses['train']}, Validation loss: {losses['val']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "-i1bXeRS4TEF",
      "metadata": {
        "id": "-i1bXeRS4TEF"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "def _gen_step(carry, rng):\n",
        "    window, = carry  # window: (B, block_size)\n",
        "    logits = model.apply(params, window)\n",
        "    next_token = jax.random.categorical(rng, logits[:, -1, :])  # (B,)\n",
        "    new_window = jnp.concatenate([window[:, 1:], next_token[:,None]], axis=1) #add next_token[:,None] here instead\n",
        "    return (new_window,), next_token #return next_token without adding extra dimension\n",
        "\n",
        "\n",
        "@partial(jax.jit, static_argnums=(3,))\n",
        "def generate_batch(params, init_idx, key, max_new_tokens: int):\n",
        "    \"\"\"\n",
        "    params:        your model params pytree\n",
        "    init_idx:      int32 array of shape (B, T0) with T0 <= block_size\n",
        "    key:           a PRNGKey\n",
        "    max_new_tokens: number of new tokens to sample\n",
        "    returns: full_seq (B, T0+max_new_tokens), all_new_tokens (max_new_tokens, B)\n",
        "    \"\"\"\n",
        "    B, T0 = init_idx.shape\n",
        "    assert T0 <= block_size, f\"Context length must be ≤ block_size ({block_size}), got {T0}\"\n",
        "\n",
        "    # left-pad init_idx up to block_size so our carry window is fixed-size\n",
        "    pad_len     = block_size - T0\n",
        "    init_window = jnp.pad(init_idx, ((0,0), (pad_len,0)), constant_values=0)  # (B, block_size)\n",
        "\n",
        "    # split RNG into one key per token\n",
        "    keys = jax.random.split(key, max_new_tokens)\n",
        "\n",
        "    # run the scan\n",
        "    (final_window,), new_tokens = jax.lax.scan(\n",
        "        _gen_step,\n",
        "        (init_window,),  # initial carry\n",
        "        keys             # scan over these RNGs\n",
        "    )\n",
        "    # new_tokens: (max_new_tokens, B)\n",
        "\n",
        "    # rebuild the full generated sequence\n",
        "    #   - take the tail of the init_window to recover the original context\n",
        "    #   - concatenate with the newly sampled tokens\n",
        "    context = init_window[:, pad_len:]               # (B, T0)\n",
        "    #gen_seq = jnp.transpose(new_tokens, (1,0))       # (B, max_new_tokens)\n",
        "    gen_seq = new_tokens.reshape(new_tokens.shape[1], new_tokens.shape[0]) # Reshape new_tokens to (B, max_new_tokens)\n",
        "    #gen_seq = new_tokens.squeeze()\n",
        "    full_seq = jnp.concatenate([context, gen_seq], axis=1)  # (B, T0 + max_new_tokens)\n",
        "\n",
        "    return full_seq, new_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "Jzk4rR5-6BFi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzk4rR5-6BFi",
        "outputId": "c54e4c35-4241-45ec-d882-01a69f691877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ISABELLA:\n",
            "I am confederate by the part.\n",
            "\n",
            "ANGELO:\n",
            "I am but sorry story in the like night\n",
            "Till hold City to Richard me:' and that he was,\n",
            "Befall'd upon the fixed her Rutland's dead?\n",
            "And now medlar, revall'd, and some name appine,\n",
            "That 'Petter nail'd to march with such as waves,\n",
            "But here crafter and so love his tears.\n",
            "\n",
            "GLOUCESTER:\n",
            "That Romeo, my lord, because I knew baby wind,\n",
            "And yet to five in the mind he that made you taughter\n",
            "so knowledge, who did not she?\n",
            "\n",
            "Third Citizen:\n",
            "God he hath raised his bawd, be deliberal.\n",
            "\n",
            "MAMILGIA:\n",
            "Go, bind my lord, given yields years, hear me speak.\n",
            "\n",
            "HERMIONE:\n",
            "What wench murder's veinstrumed well.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "'Tis like not. My dukedom, my time dead;\n",
            "You had rather been ventedious; ly which fall\n",
            "Prepared our corse; or if no more, be merry,\n",
            "But not sickliness all. He what doth a very doth.\n",
            "If he be rough with us, but I am dishonour'd;\n",
            "Of a thousand times noble Say,\n",
            "But at the mermaids: I will tide there,\n",
            "I have mine hear, I say, with pure sorrow,\n",
            "And I a\n"
          ]
        }
      ],
      "source": [
        "batch_size = 1\n",
        "init       = jnp.zeros((batch_size, 1), dtype=jnp.int32)  # start with BOS token\n",
        "key, subkey = jax.random.split(key)\n",
        "\n",
        "full_ids, tokens = generate_batch(params, init, subkey, 1000)\n",
        "print(decode(full_ids[0].tolist()))  # your generated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6eFTZ8Ue6ipD",
      "metadata": {
        "id": "6eFTZ8Ue6ipD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
