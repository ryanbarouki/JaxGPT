{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2053239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "config = GPT2Config()\n",
    "# Apparently GPT2 ties the last linear layer to the initial word embeddings\n",
    "# so the final layer is wte.embedding.T (768, 50257)\n",
    "# config.tie_word_embeddings = False\n",
    "hf_model = FlaxGPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"np\")\n",
    "outputs = hf_model(**inputs)\n",
    "\n",
    "# retrieve logts for next token\n",
    "next_token_logits = outputs.logits[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "240b65f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50257)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7423cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(params):\n",
    "    out = []\n",
    "    def print_keys_flat(params, prefix=\"\"):\n",
    "        for p in params:\n",
    "            if isinstance(params[p], dict):\n",
    "                print_keys_flat(params[p], f\"{prefix}.{p}\")\n",
    "            else:\n",
    "                out.append((f\"{prefix}.{p}\", params[p].shape))\n",
    "    print_keys_flat(params)\n",
    "    return out\n",
    "            \n",
    "def print_keys(params, offset=\"\"):\n",
    "    for p in params:\n",
    "        if not isinstance(params[p], dict):\n",
    "            print(offset+p, params[p].shape)\n",
    "        else:\n",
    "            print(offset+p)\n",
    "            print_keys(params[p], offset + \"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eed00224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, and my project will get better with time, but I think there are a lot more things that can help you\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a language model, so if I don't have a problem, I can fix it by creating new words\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'm trying to learn some stuff. I'll try to do some basic programming and just learn better ways\"},\n",
       " {'generated_text': \"Hello, I'm a language model, but I don't believe in grammar. This will work for every language model. You can define it very quickly\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, a model of how things should be, and then we look at different things as well.\" I\\'d like to'}]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b94d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanbarouki/miniconda3/envs/pytorchenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gpt2 as nn\n",
    "from importlib import reload\n",
    "reload(nn)\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "model = nn.GPT2(nn.Config())\n",
    "params = model.from_pretrained('gpt2')\n",
    "\n",
    "# model_blank = nn.GPT2(nn.Config())\n",
    "key = jax.random.PRNGKey(42069)\n",
    "# dummy = jnp.ones((1,1), dtype=int)\n",
    "# params = model.init(key,dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5248db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "block_size = nn.Config().block_size\n",
    "\n",
    "def _gen_step(carry, rng):\n",
    "    window, = carry  # window: (B, block_size)\n",
    "    logits = model.apply(params, window)\n",
    "    next_token = jax.random.categorical(rng, logits[:, -1, :])  # (B,)\n",
    "    new_window = jnp.concatenate([window[:, 1:], next_token[:,None]], axis=1) #add next_token[:,None] here instead\n",
    "    return (new_window,), next_token #return next_token without adding extra dimension\n",
    "\n",
    "\n",
    "# @partial(jax.jit, static_argnums=(3,))\n",
    "def generate_batch(params, init_idx, key, max_new_tokens: int):\n",
    "    \"\"\"\n",
    "    params:        your model params pytree\n",
    "    init_idx:      int32 array of shape (B, T0) with T0 <= block_size\n",
    "    key:           a PRNGKey\n",
    "    max_new_tokens: number of new tokens to sample\n",
    "    returns: full_seq (B, T0+max_new_tokens), all_new_tokens (max_new_tokens, B)\n",
    "    \"\"\"\n",
    "    B, T0 = init_idx.shape\n",
    "    assert T0 <= block_size, f\"Context length must be â‰¤ block_size ({block_size}), got {T0}\"\n",
    "\n",
    "    # left-pad init_idx up to block_size so our carry window is fixed-size\n",
    "    pad_len     = block_size - T0\n",
    "    init_window = jnp.pad(init_idx, ((0,0), (pad_len,0)), constant_values=0)  # (B, block_size)\n",
    "\n",
    "    # split RNG into one key per token\n",
    "    keys = jax.random.split(key, max_new_tokens)\n",
    "\n",
    "    # run the scan\n",
    "    (final_window,), new_tokens = jax.lax.scan(\n",
    "        _gen_step,\n",
    "        (init_window,),  # initial carry\n",
    "        keys             # scan over these RNGs\n",
    "    )\n",
    "    # new_tokens: (max_new_tokens, B)\n",
    "\n",
    "    # rebuild the full generated sequence\n",
    "    #   - take the tail of the init_window to recover the original context\n",
    "    #   - concatenate with the newly sampled tokens\n",
    "    context = init_window[:, pad_len:]               # (B, T0)\n",
    "    #gen_seq = jnp.transpose(new_tokens, (1,0))       # (B, max_new_tokens)\n",
    "    gen_seq = new_tokens.reshape(new_tokens.shape[1], new_tokens.shape[0]) # Reshape new_tokens to (B, max_new_tokens)\n",
    "    #gen_seq = new_tokens.squeeze()\n",
    "    full_seq = jnp.concatenate([context, gen_seq], axis=1)  # (B, T0 + max_new_tokens)\n",
    "\n",
    "    return full_seq, new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e41e2a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,\n",
       "          406, 12582,  8053,   318,   257,  3303,  2746,    13,   770,\n",
       "          318,   262,  3061,   286,   428,  1492,    13,  3914,   338,\n",
       "          766,   352,    13, 14365, 43506,   279,  6619,   546,   362,\n",
       "           13, 14365, 10888, 36883, 27992,   513,    13, 14365, 32448,\n",
       "         2420, 32096,   604,    13, 13610,   617,  3621,   513,    35,\n",
       "         5563,   642,    13, 10934,  1223,   422,   262,  2323,   510,\n",
       "          198, 29800,    12, 16129,  1486,   198, 20570, 31026,   198,\n",
       "        27871,   320,  5612,   286,  3797,  6570, 23154,   290,  2296,\n",
       "          315,  1799,   287,  7386, 34175,   198,   464, 16585, 15806,\n",
       "         1781,    13,   198, 41730,  3498, 32144,   198, 41730,  3498,\n",
       "          481,  6486,   287,   262,  4569,   314,    14,    46,  1490]],      dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inputs = jnp.array(inputs['input_ids'])\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "key, gen_key = jax.random.split(key)\n",
    "start = \"Hello, I'm a language model,\"\n",
    "inputs = jnp.array(enc.encode(start)).reshape(1,-1)\n",
    "out = model.generate(key, params, inputs, 100)\n",
    "# out, tokens = generate_batch(params, inputs, gen_key, 2)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdcf4e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a language model, and LazyList is a language model. This is the goal of this book. Let's see 1. Learn ruby p talked about 2. Learn Ruby scripting tutorials 3. Learn Scheme text parsing 4. Create some nice 3D objects 5. Build something from the ground up\n",
      "Multi-language design\n",
      "Getting Started\n",
      "Optimisation of cataphysics and immutability in domain constructs\n",
      "The chemistry classroom course.\n",
      "Learning Lab Steps\n",
      "Learning Lab will lie in the traditional I/O vis\n"
     ]
    }
   ],
   "source": [
    "print(enc.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61902f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
